# Smart AI Spectacle â€” Assistive Wearable for the Visually Impaired  
_A project by Gokul Subedi, Samir Bhattarai, and Krish Gurung_

---

## ğŸ¥ Demo Showcase

### ğŸ“Œ What Was Planned  
ğŸ§  Our original vision included a complete visually impaired support Including - Reading Assistant, Scene Description, Face Recognition and Nepali Currency Identification.

ğŸ¯ Watch our planned concept video:  
ğŸ‘‰ [ğŸ“¹ Vision & Planned Video](https://drive.google.com/file/d/1bvSfbsYHHhIE6tTSr1Md4ksHpTwNekQm/view?usp=drive_link)

ğŸ“š The book reading part of this vision evolved into a standalone project:
by _Arjun & Sushmeet_

ğŸ‘‰ **[Book Reader â€” See Repository](https://github.com/MeSafal/readingAssistant)**

---

### âœ… What Was Achieved  
ğŸš€ Hereâ€™s what we successfully built and demonstrated:  
ğŸ‘‰ [ğŸ“½ï¸ Project Demo Video](https://drive.google.com/file/d/1WOk6OGF2B9G4DnmsZEBukvpV3pstKnXK/view?usp=drive_link)

---

### ğŸ‰ Fun Behind the Scenes  
ğŸ˜„ Engineering has its moments â€” hereâ€™s a lighthearted look behind the build process:  
ğŸ‘‰ [ğŸ¬ Watch Fun/Bloopers Video](https://drive.google.com/file/d/1OVX2-oNFI8HZVuiPecNrl6nQ6l_yAgpX/view?usp=drive_link)

---


## ğŸï¸ Project Video Library

> ğŸ’¡ Explore everything me and my team have built â€” from our early vision to final execution and fun behind-the-scenes moments.

ğŸ‘‰ [ğŸ“ Access All Project Videos on Google Drive](https://drive.google.com/drive/folders/1XJ2Pygg4lbet_aBQyB7b7QG9pEV38vye?usp=drive_link)

_This shared folder contains planning, demo, and fun recordings for full transparency and storytelling._

---


## ğŸ“œ Overview

The **Smart AI Spectacle** is a revolutionary wearable device that assists visually impaired individuals by providing real-time audio feedback about their surroundings, identifying Nepalese currency, and recognizing familiar faces. Developed as a Bachelorâ€™s degree project at Tribhuwan University, Nepal, it leverages advanced AI models (ResNet-18, ResNet-50, and LSTM/Blip) and an ESP32-Cam to process images, delivering audio descriptions via a Text-to-Speech (TTS) engine. This innovative solution enhances independence and accessibility, turning eyewear into a smart guide that describes the environment, detects money, and identifies peopleâ€”all through seamless hardware and software integration.

For full technical details, see the [Project Report](final_project_smart_AI_spectacle.pdf).

## ğŸ¥ Demo

- ![Placeholder for Hardware Image] (Stunning eyewear setup image to be added.)
- ![Placeholder for Sample Output Image] (Example of currency/face recognition output to be included.)


All demo assets will be in the `images/` folder.  
_Assets are from [Project Report](final_project_smart_AI_spectacle.pdf), may be a bit blurry._

---
## âœ¨ Features

- **Real-time Scene Description** with ResNet-50 and LSTM/Blip
- **Currency Identification** using ResNet-18 for Nepalese denominations
- **Face Recognition** with ResNet-50 and custom Haar cascade detection
- **Wireless Communication** via ESP32-Cam WiFi hotspot
- **Audio Feedback** via pyttsx3 TTS engine
- **Modular Design**: Sender (ESP32-Cam) / Receiver (laptop) architecture

---
## ğŸ“¦ System Operation Phases

This project is built around a robust **Masterâ€“Slave architecture**, split into two core functional phases:

- ğŸ›°ï¸ **Slave Phase** â€“ Managed by the ESP32-CAM (image capture + communication)
- ğŸ§  **Master Phase** â€“ Managed by the Laptop (image processing + AI inference + TTS)

---

### ğŸ›°ï¸ Slave Phase â€” ESP32-CAM Logic

The ESP32-CAM acts as a dedicated lightweight image sender and input handler. Its responsibilities are minimal but mission-critical.

#### ğŸ” Always-On Listener Mode
- The ESP32 remains in an idle loop, doing nothing until a **physical button press** is detected.
- Upon the first press, it immediately enters the **Active Zone**.

#### ğŸ”— Signal to Master
- After activation, ESP sends a **notification signal** to the laptop (Master), indicating readiness and button press count.
- It then waits for a formal **image request** from the laptop.

#### ğŸ”” Button Interrupt Handling
- Each button press is **counted**, starting from 1.
- Multiple presses during this state are logged without interfering with image capture.

#### ğŸ“· Image Capture & API Delivery
- Once the master sends the image request, ESP:
  - Captures a new image
  - Sends it via a secured API call (with the button press count in the header)
  - Returns immediately to the idle loop

#### ğŸ•’ Timeout Logic
- If the laptop doesnâ€™t respond within **1 second**, ESP:
  - Assumes the master is busy or unreachable
  - Cancels the interaction
  - Flushes the button count
  - Returns to idle mode (no image is captured)

#### ğŸ“¡ Offline-Ready Communication
- ESP and laptop operate via their **own hosted WiFi hotspot**.
- The system does **not rely on external networks** or internet connectivity.
- This design ensures full functionality even in remote or disconnected environments â€” like dense urban areas, rural villages, or even forested zones.

ğŸ“Œ This lightweight, interrupt-driven approach ensures ESP remains **responsive**, **efficient**, and always ready to serve when the master system is available.

## ğŸ§  Master Phase â€” Laptop Logic

Upon receiving the ESP32-CAMâ€™s button-press interruption, the laptop (Master) orchestrates the heavy lifting: model selection, image request, inference, and audio output.

---

### â±ï¸ Debounce & Mode Selection (0.9 s Window)
- **Wait 0.9 seconds** after the first interrupt to collect additional presses:
  - **1 press** â†’ Face Recognition  
  - **2 presses** â†’ Currency Identification  
  - **3+ presses** â†’ Scene Description  
- This 0.9 s window ensures all user intents arrive before the ESP times out at 1 s.

---

### ğŸ”— Image Request
- After 0.9 s, Master sends an **â€œImage Requestâ€** signal back to the ESP.
- ESP then captures and streams the image + press count via the secured API.

---

### ğŸ–¼ï¸ Image Reception & Preprocessing

1. **Reception**  
   - Receives the raw image data and associated button-press count via API.

2. **Preprocessing**  
   - Converts the image into a model-compatible format using the **same preprocessing pipeline** applied during model training (e.g., resizing, normalization, color-space conversion).

3. **Implementation Detail**  
   - For added robustness and modularity, preprocessing logic may reside within individual model pipelines. This ensures each model explicitly applies its expected preprocessing steps before inference.


---

### ğŸ¤– Model Inference
- **Route** image to the correct AI model based on button count:
  1. Face Recognition  
  2. Money Identification  
  3. 3+ Scene Description  
- **Await** model output (all models run locally on the laptop).

---

### ğŸ”Š Audio Feedback
- **Convert** the modelâ€™s text output to speech via the default playback engine.  
- **Play** audio on the active speaker in a separate thread, so UI remains responsive.

---

### ğŸš« Interrupt Handling
- **While processing**, any new ESP interrupts are **ignored**:
  - Master does **not** send a new request, causing ESP to timeout and flush.  
- **Exception**: During longer Scene Description inference, rapid presses can preempt a new cycleâ€”demonstrating robust ignore logic.

---

### ğŸ”„ Fallback & Flexibility
- **No ESP?** Master can:
  1. Use any connected camera (e.g., USB webcam)  
  2. Accept keyboard input (`1`, `2`, `3`) to select mode  
- This â€œoffline, hardware-agnosticâ€ design proves Masterâ€™s versatility beyond the ESP32 hotspot.

---

> ğŸ“Œ **Note:** The Masterâ€“Slave handshake (0.9 s wait + 0.1 s ESP timeout) ensures reliable, user-friendly operation in any environmentâ€”urban, rural, or off-grid.  

## ğŸ› ï¸ Hardware Components

| Component          | Description                                 |
|--------------------|---------------------------------------------|
| ESP32-Cam          | Captures images and handles button inputs   |
| Push Buttons       | Controls power supply and workflow          |
| Buck Converter     | Regulates power for stable operation        |
| Speaker            | Delivers audio feedback                     |
| Eyewear            | Houses the impressive hardware setup        |
| Laptop             | Processes AI models with CUDA-capable GPU   |

## ğŸ’» Software & Libraries

- **Python 3.x**
- **TensorFlow** (CUDA 12.6 compatible)
- **OpenCV**, **NumPy**, **PyTorch**, **transformers**, **pyttsx3**
- C/C++ for ESP32 programming
- Visual Studio Code and Arduino IDE for development

## ğŸ—ºï¸ System Architecture & Flowchart

- ![System Block Diagram](images/system.png)
- ![Transmitter](images/transmitter.png)
- ![Hardware Simplified](images/hardware.png)


```text
ESP32-Cam â†’ Image Capture â†’ WiFi â†’ Laptop â†’ AI Models â†’ TTS â†’ Speaker â†’ Audio Feedback
```

## ğŸ”„ Control Mapping & Safety

| Button Press       | Action                          |
|--------------------|---------------------------------|
| 1 Press            | Activate Face Recognition       |
| 2 Presses          | Activate Currency Identification|
| 3+ Presses         | Activate Scene Description      |

**Safety**: Power button ensures controlled activation/deactivation of the ESP32-Cam.

---
## ğŸ“¡ Communication & Control Functions

```python
# (High-level functions implied, code available in repo)
# Image processing and audio generation handled by master.py
```

## ğŸ“ˆ Model Performance

- **Currency Identification**: ~98% validation accuracy, with rare misclassifications.
- **Face Recognition**: High accuracy for known faces, with some overfitting.
- **Scene Description**: Strong BLEU scores, though validation loss plateaus slightly.

## ğŸš§ Limitations & Future Work

- **Prototype Only**: Tested in controlled environments.
- **Laptop-Dependent**: Requires a CUDA-capable GPU.
- **No Offline Mode**: Relies on WiFi and laptop processing.
- **Limited Dataset**: Further data collection needed for edge cases.

**Next Steps**:  
- Add offline processing capability
- Enhance model accuracy with larger datasets
- Develop a standalone wearable unit
- Improve battery life and portability

## ğŸ“ Project Status

Proof of Concept with real-time audio feedback. Ready for user testing and hardware optimization.

## ğŸ‘¥ Project Contributors

_All contributors collaborated closely on all aspects of the project, from concept to implementation._

- **Gokul Subedi** â€” Software Engineer & System Designer  
- **Samir Bhattarai** â€” Hardware Engineer & System Designer  
- **Krish Gurung** â€” Data Scientist & System Designer  

## ğŸ¤ Supported By

> **Arjun Koirala and Sushmeet Paudel**  
> _Pillar of Strength & Development Champions_  

This project proudly celebrates the exceptional support, dedication, and invaluable contributions of **Arjun Koirala and Sushmeet Paudel**, whose efforts have been a cornerstone of our development and deployment success.

---

## ğŸ¤ License & Contribution

Â©ï¸ All rights reserved by Gokul Subedi, Samir Bhattarai, and Krish Gurung.  
Licensed under the [MIT License](https://opensource.org/licenses/MIT).

If youâ€™re eager to collaborate, offer support, or share suggestions, my team and I are open to working with you. Reach out at [github.com/meSafal](https://github.com/meSafal) or [subedigokul119@gmail.com](mailto:subedigokul119@gmail.com).

---

## ğŸ« Acknowledgments

- Arjun Koirala and @Sushmit Paudel for exceptional support. 
- Kaggle community and open-source contributors

---

## ğŸ“Œ Project Status

- âœ… Prototype deployed and tested successfully
- âœ… Real-time performance with offline playback
- ğŸ”„ Roadmap includes dynamic TTS and support for mobile deployment

---

> âš ï¸ This is a public-sharing copy. Dataset and select training scripts are withheld.

> âš ï¸ All images in here are added from the project report itself.

<sub><p align="center">ğŸ“˜ This repository and its documentation were prepared and maintained by <a href="https://github.com/MeSafal/" target="_blank"><u><strong>Gokul Subedi</strong></u></a>. For the latest updates and related projects, visit <a href="https://github.com/MeSafal" target="_blank">github.com/MeSafal</a>.</p></sub>
